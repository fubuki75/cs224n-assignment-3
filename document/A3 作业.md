# A3 dependency parsing

1. `parser_model.py  ` 

   该文件基于`pytorch`构建简单的神经网络模型`ParserModel`，其中曾涉及到的之前没用过的`nn.Embedding()`。实现模型的内部层的定义与初始化，和前馈过程。本任务为分类任务，最终输出为类似one-hot形式的logits，不用softmax，在forward中确定哪类transition概率最大。

2. `parser_transitions.py`

   实现了`PartialParse`类，该对象存储单个句子的同时，包含了stack，buffer和dependencies，但这个类在整体的代码中没有起到作用，只是作为测试的中间过程存在。内部函数：

   * `parse_step`接收单步操作，更新stack，buffer，dependencies
   * `parse`接收操作list，将一个句子完全parse好

   定义了`minibatch_parse`函数能够通过minibatch，一步步地把batch内的句子边预测transition边parse好，是minibatch训练的关键函数之一。

   以及很多检测函数，我觉得写得不错。

3. `run.py`

   整个模型的训练和检验，串联每个部分，数据预处理、训练、测试。这里使用了`tqdm`库可视化训练的过程。分开定义了train函数和train_for_epoch。

4. `parser_utils.py`

   本次实验使用的 数据 主要是`.conll`文件和，但这个对于将每次transition视为分类任务的neural dependency parser来说，y_transition是没有的，得自己生成。

   `.conll`文件是用于存储经过标记的句子集，而存在的格式，以空行来区分不同句子，空格来区分不同列，每一行代表一个单词，[这篇帖子对字段的解释非常好](https://blog.csdn.net/gammag/article/details/78523053)

   本文用到的`.conll`文件的格式为：
   #0 id索引
   #1 word或者是标点
   #2 词形的词条或词干
   #3 google提取的通用POS
   #4 语言特定的词性标签，下划线为不可用
   #5 特征列表，下划线为不可用
   #6 指向当前词dependencies的发出者的编号(0代表root)，表示依赖于...
   #7 该词与head的依赖关系种类
   #8 二级依赖列表
   #9 其他注释

   ```python
   class Parser(object):#一个语料库的数据对应一个Parser对象
       def __init__(self,dataset)
       #初始化，Parser类大量数据成员,根据语料库将token（单词，pos label，dependency label）附上id，同时不忘了未知（unk）、补位空（NULL）以及根（ROOT）。
       def vectorize(self,examples)
       #将examples（list of dicts，每一句话单独一个dict，里面包含word，pos，head，label四个数组，但里面装的都是真实的名称等等）转化为index，vectorize。
       def extract_features(self,stack,buf,arcs,ex)
       #根据当前的stack，buf和提取出的arcs，提取特征。
       def get_oracle(self,stack,buf,ex)
       #生成y_transition，也就是gold_t
       def create_instances(self,examples)
       #将gold_t与真实的移动情况相结合
       def legal_labels(self,stack,buf)
       #用于检验gold_t生成的对不对的？？
       def parse(self,dataset,eval_batch_size)
       #用于检测UAS
   class ModelWrapper(object):
       def predict(self,partial_parses)
       #用于预测下一步的transition
   def read_conll(in_file,lowercasr=False,max_example=None):
       #最关键的读取数据
   def build_dict(keys,n_max=None,offset=0):
       #帮助建立tok2id
   def punct(language, pos):
       #暂不明确
   def minibatches(data, batch_size):
       #函数用于从data中分开输入特征和y_transition
   def load_and_preprocess_data(reduced=True):
       #所有预处理工作都在这里，串联了大量parser_utils.py文件中的函数，读取数据，建立Parser，载入pretrained embeddings，vectorizing data，preprocessing training data等等。
   ```

# 其他收获

1. `nn.Embedding()`一般用于存储word embeddings，然后通过索引来调取对应单词的嵌入.

   * 定义nn.Embedding()：*ATTENTION！*参数`padding_index=2`指的是第几个index代表占位符，使得句子间等长。

   * 建立好nn.Embedding()对象后：输入：(\*)怎么样都行，输出为：(*,embedding_size)

   * 使用示例：

     ```python
     #先定义整体的shape
     self.pretrained_embeddings = nn.Embedding(embeddings.shape[0],self.embed_size)
     #再赋值，embeddings是shape为(token的数量，50：词向量维度数)的np.array,第i行是tok2id中id==i的token对应的嵌入
     self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))
     #如何应用？其中t是shape为(batch_size,n_features),n_features必须为index序列，输出x为(batch_size,n_features,self.embed_size)
     x = self.pretrained_embeddings(t)
     ```

   - [这篇帖子讲的不错](https://www.jianshu.com/p/63e7acc5e890)

     结合RNN推荐用\[seq_len,batch_size] (一般由[batch_size,seq_len]转换而来)，

     每个单词用index代替，并加上了结束符号EOS，进行了填补（每个样本等长，填补方法用了其他函数），格式为LongTensor

2. 列表生成式的灵活运用

   全文几乎完全看不到简单的循环，存在大量的列表生成式，简洁明了。

   列表生成式：`[x for x in range(1,50)]`中，for...if后面不可加else，if...else

   ```python
   #下面这句对最终的输出进行了控制，在后面也使用if else和for机制
   [("RA" if pp.stack[1] is "right" else "LA") if len(pp.buffer) == 0 else "S"  for pp in partial_parses]`这里对简单if和[x for]的运用很灵性
   ```

3. `zip`和`enumerate`两个for循环中常用条件

   ```python
   for i,j in zip(a,b)#打包a和b的元素，每次各出一个
   for i in enumerate(a)#输出a的元素，还加上了序号，可以计数
   for i,j in enumerate(zip(a,b))#打包a和b的元素，同时加上序号
   ```

4. `tqdm`库

   可用于以进度条的形式可视化这个过程。

   ```python
   from tqdm import tqdm
   with tqdm(total=100) as prog:
       #函数体
       for i in range(100):
           prog.update(1)#循环中每次更新一个
   
   for src_sent in tqdm(test_data_src,desc='Decoding'): #desc是对进度条的文字说明，test_data_src只要是iterable
   #这样反而可以直接反映出for循环的直行进度
   ```

5. 简单“重复”字符/数组的快速生成

   ```python
   [1]*80
   90*'='
   ```

6. 输出文件到某个目录

   如果怕目录不存在的话，可以自动创建

   ```pyt
   if not os.path.exists(output_dir):
           os.makedirs(output_dir)
   ```

7. `collection`python自带库

   `collections`库中的`Counter`类

   ```python
   from collections import Counter
   a=['A','A','B','C']
   counter=Counter(a)#返回key为a中的元素，value为频数的字典
   counter.most_common(k)#前k个高频词，返回的是[()]list of tuples
   ```

   `collection`库中的[`namedtuple`]( https://www.cnblogs.com/linwenbin/p/11282492.html)

   ```python
   def namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)
   ```

   * 可以产生一个可读性更强的tuple
   * namedtuple函数返回的是一个tuple的子类对象，
   * 不仅仅可以满足普通tuple中通过index a[0]来调用，还可以通过tuple的每个字段名称a.score访问，
   * 与普通tuple占用内存一致

   ```python
   from collections import namedtuple
   Hypothesis = namedtuple('Hypothesis', ['value', 'score'])#定义Hypothesis类，该类具有value和score两个元素
   a=Hypotheis(1,2)
   x,y=a#可正常解包
   ```

8. for...else...结构

   for...else...语法，for执行完了之后执行else；如果在for中遇到了`break`，则不再执行`else`

   ```python
   for i in range(n_words*2):
       pass
   else:
       #代码
   ```

9. with...as...

   经常用于使用open函数，这样可以自动帮关闭文件;

   as的作用是`f=open(in_file)`

   ```python
   with open(in_file) as f:
   
   try:
   except:
   finally:
   ```

10. dict字典的update函数，能够更新key:value，更改/新增，但不能删减

11. 计算某一步的时间

    ```python
    import time
    start=time.time()
    #待测语句
    time.time()-start
    ```

12. [`np.asarray()`和`np.array()`的区别](https://blog.csdn.net/fu6543210/article/details/83242047)

    ```python
    a=np.array([1,2,3,4,5])
    b=np.array(a)#b由a复制得到
    c=np.asarray(a)#a不用复制即可得到c；但如果使用asarray的其他参数，比如更改了数据类型，则也是复制得到的
    ```

    

